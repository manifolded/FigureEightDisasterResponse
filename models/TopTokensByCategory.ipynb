{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sqal\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = sqal.create_engine('sqlite:///../data/DisasterResponse.db')\n",
    "df = pd.read_sql_table('MessageCategorization', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct outlying values in `related` column\n",
    "df['related'] = np.clip(df['related'], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_columns = 'message'\n",
    "out_columns = list(df.columns)[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[in_columns].values\n",
    "y = df[out_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = train_test_split(text, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://realpython.com/natural-language-processing-spacy-python/ was helpful here\n",
    "def tokenize(text):\n",
    "    doc = en_nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if token not in stopwords and not token.is_punct]\n",
    "    stems = [stemmer.stem(lemma) for lemma in lemmas]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=tokenize, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keith/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(min_df=5, tokenizer=<function tokenize at 0x126becdc0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vect.fit(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 1.39 s, total: 1min 33s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_pred = vect.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8652, 4859)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is 8652 samples in the test set, and 4859 tokens in the vectorizer output vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract The Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there some other way we could get the vocabulary?  Some way where we don't have to reconstruct and train the vectorizer?\n",
    "- Can we store the vectorizer like we store the model?  Then we could just read it from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = vect.vocabulary_\n",
    "vocab = list(vect.vocabulary_.keys())\n",
    "# No good.  vocabulary_ is a dict which is unordered.  We can't rely on it to return the tokens in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4859"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_voc = len(vocab)\n",
    "n_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct Model (without vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    MultiOutputClassifier(\n",
    "        estimator=AdaBoostClassifier(\n",
    "            base_estimator=DecisionTreeClassifier(max_depth=2),\n",
    "            n_estimators=10, learning_rate=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open('severedPipelineXtrain.pkl', 'rb')\n",
    "X_train = pkl.load(fileObject)\n",
    "fileObject.close()\n",
    "\n",
    "fileObject = open('severedPipelineXtest.pkl', 'rb')\n",
    "X_test = pkl.load(fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31 s, sys: 330 ms, total: 31.3 s\n",
      "Wall time: 35.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('multioutputclassifier',\n",
       "                 MultiOutputClassifier(estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),\n",
       "                                                                    learning_rate=1,\n",
       "                                                                    n_estimators=10)))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Canon Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we're up to...   For every token in the vocabulary we're going to take its vector representation (a single 1 and a lot of 0s) and feed it to the pipeline to get a prediction.  That prediction becomes a row with 36 elements which is appended to a big table which winds up with 4859 rows.  We get a big table 4859 x 36.\n",
    "\n",
    "From this table we then extract the columns.  For each non-zero entry in a column we look up the associated token.  We call these tokens the \"canon tokens\".  \n",
    "\n",
    "What we are doing is a crude kind of matrix inversion, which would give us our features perfectly if the pipeline were a strictly linear system, but of course it isn't.  But it's the best we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTokenTable(n_vocab):\n",
    "    result = np.zeros(shape = (n_vocab, 36))\n",
    "    for i in range(n_vocab):\n",
    "        # construct token vector for single token\n",
    "        tokenVec = np.zeros(shape=n_vocab)\n",
    "        tokenVec[i] = 1\n",
    "        # compute categories for that single token and append to table\n",
    "        result[i] = pipe.predict([tokenVec])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Rewrite `genTokenTable()` to use csr (or lil) rather than np.array.~~  Ok. That was a terrible idea.  It's non-trivial to use sparse.lil arrays with the exceptionally useful `numpy.where()` function.  Let's go back to the original method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def genTokenTable(n_vocab):\n",
    "#     result = sparse.lil_matrix((n_vocab, 36), dtype=int)\n",
    "#     for i in range(n_vocab):\n",
    "#         # construct token vector for single token\n",
    "#         tokenVec = np.zeros(shape=n_vocab)\n",
    "#         tokenVec[i] = 1\n",
    "#         # compute categories for that single token and append to table\n",
    "#         result[i,:] = pipe.predict([tokenVec])\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 4s, sys: 2.76 s, total: 5min 7s\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenTable = genTokenTable(n_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genCanonTable(table):\n",
    "    result = []\n",
    "    for i in range(36):\n",
    "        categoryVector = tokenTable[:,i]\n",
    "        tokenIndices = np.where(categoryVector == 1)[0]\n",
    "        # A wily trick for indexing a list\n",
    "        result.append(list(np.array(vocab)[tokenIndices]))\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canonTable = [canonTokens(tokenTable, i) for i in range(36)]\n",
    "canonTable = genCanonTable(tokenTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'medan',\n",
       "  'chapter',\n",
       "  'of',\n",
       "  'taiwan',\n",
       "  'buddhist',\n",
       "  'tzu',\n",
       "  'chi',\n",
       "  'foundat',\n",
       "  'set',\n",
       "  'up',\n",
       "  'a',\n",
       "  'recept',\n",
       "  'center',\n",
       "  'at',\n",
       "  'an',\n",
       "  'indonesian',\n",
       "  'militari',\n",
       "  'base',\n",
       "  'in',\n",
       "  'on',\n",
       "  'dec',\n",
       "  '29',\n",
       "  'to',\n",
       "  'help',\n",
       "  'victim',\n",
       "  'flee',\n",
       "  'tsunami',\n",
       "  'ravag',\n",
       "  'ach',\n",
       "  'provinc',\n",
       "  'i',\n",
       "  'be',\n",
       "  'flood',\n",
       "  'port',\n",
       "  'au',\n",
       "  'princ',\n",
       "  'now',\n",
       "  'live',\n",
       "  'gonaiv',\n",
       "  'need',\n",
       "  'make',\n",
       "  'sure',\n",
       "  '-pron-',\n",
       "  'manhattan',\n",
       "  'have',\n",
       "  'emerg',\n",
       "  'food',\n",
       "  'frankenstorm',\n",
       "  'hurricanesandi',\n",
       "  'get',\n",
       "  \"'s\",\n",
       "  'mini',\n",
       "  'amp',\n",
       "  'water',\n",
       "  'for',\n",
       "  'about',\n",
       "  'these',\n",
       "  'hurrican',\n",
       "  'with',\n",
       "  'go',\n",
       "  'all',\n",
       "  'digicel',\n",
       "  'offic',\n",
       "  'find',\n",
       "  '160',\n",
       "  'can',\n",
       "  'not',\n",
       "  'one',\n",
       "  'deliveri',\n",
       "  'more',\n",
       "  'than',\n",
       "  '41',\n",
       "  'ton',\n",
       "  'therapeut',\n",
       "  'milk',\n",
       "  'and',\n",
       "  '1.5',\n",
       "  'treat',\n",
       "  'child',\n",
       "  'acut',\n",
       "  'sever',\n",
       "  'malnutrit',\n",
       "  '31',\n",
       "  'feed',\n",
       "  'centr',\n",
       "  'oper',\n",
       "  'by',\n",
       "  'unicef',\n",
       "  'partner',\n",
       "  'both',\n",
       "  'cloth',\n",
       "  'non',\n",
       "  'perish',\n",
       "  'donat',\n",
       "  'issu',\n",
       "  'warn',\n",
       "  'across',\n",
       "  'countri',\n",
       "  'includ',\n",
       "  'usual',\n",
       "  'dri',\n",
       "  'southern',\n",
       "  'south',\n",
       "  'north',\n",
       "  'yo',\n",
       "  'nigga',\n",
       "  'call',\n",
       "  'anyway',\n",
       "  '50',\n",
       "  'if',\n",
       "  'ok',\n",
       "  'so',\n",
       "  'could',\n",
       "  'start',\n",
       "  'littl',\n",
       "  'room',\n",
       "  'collect',\n",
       "  'effort',\n",
       "  '=',\n",
       "  'posit',\n",
       "  'respons',\n",
       "  'haitian',\n",
       "  'earthquak',\n",
       "  'relief',\n",
       "  'thank',\n",
       "  'everyon',\n",
       "  'support',\n",
       "  'over',\n",
       "  'http',\n",
       "  'two',\n",
       "  'brother',\n",
       "  'marri',\n",
       "  'month',\n",
       "  'befor',\n",
       "  'ruin',\n",
       "  'no',\n",
       "  'hous',\n",
       "  'student',\n",
       "  'do',\n",
       "  'card',\n",
       "  'too',\n",
       "  'leav',\n",
       "  'homeless',\n",
       "  'stress',\n",
       "  'migrat',\n",
       "  'rural',\n",
       "  'popul',\n",
       "  'supplement',\n",
       "  'suppli',\n",
       "  'wild',\n",
       "  'still',\n",
       "  'continu',\n",
       "  'compound',\n",
       "  'disast',\n",
       "  'also',\n",
       "  'bring',\n",
       "  'fatal',\n",
       "  'infect',\n",
       "  'kenya',\n",
       "  'red',\n",
       "  'cross',\n",
       "  'send',\n",
       "  'item',\n",
       "  'such',\n",
       "  'as',\n",
       "  'plastic',\n",
       "  'sheet',\n",
       "  'blanket',\n",
       "  'various',\n",
       "  'affect',\n",
       "  'area',\n",
       "  'river',\n",
       "  'garissa',\n",
       "  'district',\n",
       "  'islamabad',\n",
       "  '11',\n",
       "  '2007',\n",
       "  'xinhua',\n",
       "  'via',\n",
       "  'comtex',\n",
       "  'scorch',\n",
       "  'heat',\n",
       "  'kill',\n",
       "  'anoth',\n",
       "  '63',\n",
       "  'peopl',\n",
       "  'saturday',\n",
       "  'pakistan',\n",
       "  'remain',\n",
       "  'grip',\n",
       "  'heatwav',\n",
       "  'level',\n",
       "  'touch',\n",
       "  'record',\n",
       "  'height',\n",
       "  'number',\n",
       "  'citi',\n",
       "  'newspap',\n",
       "  'say',\n",
       "  'monday',\n",
       "  'this',\n",
       "  'villag',\n",
       "  'own',\n",
       "  'which',\n",
       "  'those',\n",
       "  'hardship',\n",
       "  'right',\n",
       "  'file',\n",
       "  'burden',\n",
       "  'hi',\n",
       "  'would',\n",
       "  'like',\n",
       "  'join',\n",
       "  'famili',\n",
       "  'usa',\n",
       "  'home',\n",
       "  'distribut',\n",
       "  'where',\n",
       "  'assess',\n",
       "  'mission',\n",
       "  'provid',\n",
       "  'local',\n",
       "  'health',\n",
       "  'author',\n",
       "  'medic',\n",
       "  'evacu',\n",
       "  '10',\n",
       "  'serious',\n",
       "  'injur',\n",
       "  'attent',\n",
       "  'sister',\n",
       "  'let',\n",
       "  'stay',\n",
       "  '3',\n",
       "  'day',\n",
       "  'instead',\n",
       "  'take',\n",
       "  'god',\n",
       "  'there',\n",
       "  'deliv',\n",
       "  'multipl',\n",
       "  'micronutri',\n",
       "  'tablet',\n",
       "  'along',\n",
       "  'sinc',\n",
       "  'parasit',\n",
       "  'infest',\n",
       "  'occur',\n",
       "  'under',\n",
       "  'unsanitari',\n",
       "  'condit',\n",
       "  'that',\n",
       "  'prevail',\n",
       "  'pictur',\n",
       "  'aunt',\n",
       "  'downtown',\n",
       "  'concepcion',\n",
       "  'chile',\n",
       "  'after',\n",
       "  '8.8',\n",
       "  'lucki',\n",
       "  'long',\n",
       "  'ass',\n",
       "  'line',\n",
       "  'gas',\n",
       "  'smh',\n",
       "  'dark',\n",
       "  'nyc',\n",
       "  'town',\n",
       "  '14',\n",
       "  'januari',\n",
       "  'woman',\n",
       "  'member',\n",
       "  'nation',\n",
       "  'congress',\n",
       "  'announc',\n",
       "  'format',\n",
       "  'main',\n",
       "  'focus',\n",
       "  'ensur',\n",
       "  'fair',\n",
       "  'represent',\n",
       "  'constitut',\n",
       "  'draft',\n",
       "  'committe',\n",
       "  'bank',\n",
       "  'particip',\n",
       "  'compris',\n",
       "  'assist',\n",
       "  'rehabilit',\n",
       "  'public',\n",
       "  'infrastructur',\n",
       "  'facil',\n",
       "  'hit',\n",
       "  'particular',\n",
       "  'sector',\n",
       "  'educ',\n",
       "  'vocat',\n",
       "  'train',\n",
       "  'becaus',\n",
       "  'univers',\n",
       "  'count',\n",
       "  'initi',\n",
       "  'launch',\n",
       "  'today',\n",
       "  'valu',\n",
       "  'usd',\n",
       "  '$',\n",
       "  '500,000',\n",
       "  'will',\n",
       "  'fund',\n",
       "  'immedi',\n",
       "  'airlift',\n",
       "  'aid',\n",
       "  'cover',\n",
       "  'staff',\n",
       "  'deploy',\n",
       "  'myanmar',\n",
       "  'how',\n",
       "  'weather',\n",
       "  'depart',\n",
       "  'other',\n",
       "  'catastrophi',\n",
       "  'veri',\n",
       "  'much',\n",
       "  '1',\n",
       "  'haze',\n",
       "  'consist',\n",
       "  'smoke',\n",
       "  'condens',\n",
       "  'matter',\n",
       "  'creat',\n",
       "  'burn',\n",
       "  'organ',\n",
       "  'fuel',\n",
       "  'among',\n",
       "  'effect',\n",
       "  'reduc',\n",
       "  'visibl',\n",
       "  'understand',\n",
       "  'what',\n",
       "  'here',\n",
       "  'heavi',\n",
       "  'rain',\n",
       "  'most',\n",
       "  'southeast',\n",
       "  'asia',\n",
       "  'caus',\n",
       "  'landslid',\n",
       "  'island',\n",
       "  'sumatra',\n",
       "  'java',\n",
       "  'lunch',\n",
       "  'back',\n",
       "  'into',\n",
       "  'place',\n",
       "  'till',\n",
       "  'finish',\n",
       "  'fix',\n",
       "  'someth',\n",
       "  'laptop',\n",
       "  'trap',\n",
       "  'insid',\n",
       "  'first',\n",
       "  'time',\n",
       "  'rememb',\n",
       "  'see',\n",
       "  'sign',\n",
       "  'turn',\n",
       "  'off',\n",
       "  'sandi',\n",
       "  'play',\n",
       "  'destroy',\n",
       "  'way',\n",
       "  'swim',\n",
       "  'park',\n",
       "  'pool',\n",
       "  ' ',\n",
       "  'pleas',\n",
       "  'write',\n",
       "  'give',\n",
       "  'hope',\n",
       "  'want',\n",
       "  '4636',\n",
       "  'news',\n",
       "  'tonight',\n",
       "  'hear',\n",
       "  'lot',\n",
       "  'person',\n",
       "  'precis',\n",
       "  'essenti',\n",
       "  'nutrit',\n",
       "  'niger',\n",
       "  'fall',\n",
       "  'from',\n",
       "  'grow',\n",
       "  'enough',\n",
       "  'even',\n",
       "  'export',\n",
       "  'cereal',\n",
       "  'state',\n",
       "  'chronic',\n",
       "  'shortag',\n",
       "  'due',\n",
       "  'recurr',\n",
       "  'drought',\n",
       "  'becom',\n",
       "  'frequent',\n",
       "  'last',\n",
       "  'decad',\n",
       "  'when',\n",
       "  'quak',\n",
       "  'strike',\n",
       "  'run',\n",
       "  'away',\n",
       "  'field',\n",
       "  'outsid',\n",
       "  'but',\n",
       "  'hill',\n",
       "  'near',\n",
       "  'road',\n",
       "  'shake',\n",
       "  'collaps',\n",
       "  'school',\n",
       "  'deputi',\n",
       "  'princip',\n",
       "  'tell',\n",
       "  'jakarta',\n",
       "  'post',\n",
       "  'delma',\n",
       "  '33',\n",
       "  'tent',\n",
       "  'care',\n",
       "  'aircraft',\n",
       "  'full',\n",
       "  'medicin',\n",
       "  'northern',\n",
       "  'afghan',\n",
       "  'mazar',\n",
       "  'e',\n",
       "  'sharif',\n",
       "  'bad',\n",
       "  'scene',\n",
       "  'massiv',\n",
       "  'destruct',\n",
       "  'strong',\n",
       "  'aftershock',\n",
       "  'keep',\n",
       "  'resid',\n",
       "  'worker',\n",
       "  'nervous',\n",
       "  'same',\n",
       "  'economi',\n",
       "  'smuggl',\n",
       "  'drug',\n",
       "  'product',\n",
       "  'terror',\n",
       "  'illeg',\n",
       "  'afghanistan',\n",
       "  'taliban',\n",
       "  'direct',\n",
       "  'alleg',\n",
       "  'goal',\n",
       "  'pakistani',\n",
       "  'chief',\n",
       "  'execut',\n",
       "  'general',\n",
       "  'musharraf',\n",
       "  'flash',\n",
       "  'sudden',\n",
       "  'rise',\n",
       "  'temperatur',\n",
       "  '20',\n",
       "  '22',\n",
       "  'degre',\n",
       "  'celsius',\n",
       "  '35',\n",
       "  '36',\n",
       "  'glacier',\n",
       "  'melt',\n",
       "  'rt',\n",
       "  'presid',\n",
       "  'radio',\n",
       "  'govern',\n",
       "  '7,000',\n",
       "  'soldier',\n",
       "  'report',\n",
       "  'thatta',\n",
       "  'come',\n",
       "  'belong',\n",
       "  'out',\n",
       "  'ngo',\n",
       "  'ani',\n",
       "  'ration',\n",
       "  'flour',\n",
       "  'oil',\n",
       "  'exhaust',\n",
       "  '15',\n",
       "  'breakfast',\n",
       "  'tea',\n",
       "  'bake',\n",
       "  'bread',\n",
       "  'while',\n",
       "  'fri',\n",
       "  'occasion',\n",
       "  'small',\n",
       "  'portion',\n",
       "  'meat',\n",
       "  'some',\n",
       "  'sell',\n",
       "  'may',\n",
       "  'requir',\n",
       "  'tropic',\n",
       "  'coastal',\n",
       "  'ecosystem',\n",
       "  'natur',\n",
       "  'insur',\n",
       "  'mechan',\n",
       "  'surviv',\n",
       "  'storm',\n",
       "  'wave',\n",
       "  'typhoon',\n",
       "  'larg',\n",
       "  'ethnic',\n",
       "  'who',\n",
       "  'tension',\n",
       "  'present',\n",
       "  'frontier',\n",
       "  'site',\n",
       "  'howev',\n",
       "  'major',\n",
       "  'connect',\n",
       "  'resum',\n",
       "  'traffic',\n",
       "  'telecommun',\n",
       "  'servic',\n",
       "  'restor',\n",
       "  'put',\n",
       "  'power',\n",
       "  'system',\n",
       "  'abl',\n",
       "  'safe',\n",
       "  'restart',\n",
       "  'conflict',\n",
       "  'soon',\n",
       "  'return',\n",
       "  'face',\n",
       "  'task',\n",
       "  'repair',\n",
       "  'recoveri',\n",
       "  'tube',\n",
       "  'well',\n",
       "  'sanitari',\n",
       "  'subsequ',\n",
       "  'threat',\n",
       "  'waterborn',\n",
       "  'epidem',\n",
       "  'cap',\n",
       "  'often',\n",
       "  'shabell',\n",
       "  'through',\n",
       "  'extens',\n",
       "  'complet',\n",
       "  'cut',\n",
       "  'offici',\n",
       "  'express',\n",
       "  'appreci',\n",
       "  'around',\n",
       "  '4',\n",
       "  'vaccin',\n",
       "  'antibiot',\n",
       "  'prompt',\n",
       "  'vital',\n",
       "  'air',\n",
       "  'civilian',\n",
       "  'plane',\n",
       "  'use',\n",
       "  'transport',\n",
       "  'hello',\n",
       "  'gover',\n",
       "  'look',\n",
       "  'should',\n",
       "  'mani',\n",
       "  'conduct',\n",
       "  'aerial',\n",
       "  'bomb',\n",
       "  'raid',\n",
       "  'sudan',\n",
       "  'violat',\n",
       "  'humanitarian',\n",
       "  'ceas',\n",
       "  'fire',\n",
       "  'juli',\n",
       "  '1998',\n",
       "  'comprehens',\n",
       "  'august',\n",
       "  '1999',\n",
       "  'jacmel',\n",
       "  'request',\n",
       "  'tractor',\n",
       "  'civil',\n",
       "  'unrest',\n",
       "  'social',\n",
       "  'disturb',\n",
       "  'nampula',\n",
       "  'disrupt',\n",
       "  'death',\n",
       "  'relat',\n",
       "  'pass',\n",
       "  'doctor',\n",
       "  'expect',\n",
       "  'handl',\n",
       "  'diseas',\n",
       "  'paediatr',\n",
       "  'case',\n",
       "  'add',\n",
       "  'angolan',\n",
       "  'opposit',\n",
       "  'parti',\n",
       "  'denounc',\n",
       "  'santo',\n",
       "  'delay',\n",
       "  'elect',\n",
       "  'origin',\n",
       "  'schedul',\n",
       "  'year',\n",
       "  'second',\n",
       "  'half',\n",
       "  '2002',\n",
       "  'risk',\n",
       "  'prolong',\n",
       "  'import',\n",
       "  'hold',\n",
       "  'credibl',\n",
       "  'sleep',\n",
       "  'camp',\n",
       "  'balakot',\n",
       "  'onc',\n",
       "  'clear',\n",
       "  'deep',\n",
       "  'muddi',\n",
       "  'terrifi',\n",
       "  'fear',\n",
       "  'debri',\n",
       "  'build',\n",
       "  'or',\n",
       "  'cancer',\n",
       "  'lead',\n",
       "  'target',\n",
       "  'candid',\n",
       "  'mobil',\n",
       "  'truck',\n",
       "  'head',\n",
       "  'down',\n",
       "  'ct',\n",
       "  'highway',\n",
       "  'donor',\n",
       "  'question',\n",
       "  'whi',\n",
       "  'ask',\n",
       "  'pay',\n",
       "  'safeguard',\n",
       "  'egg',\n",
       "  'asid',\n",
       "  'burma',\n",
       "  'leader',\n",
       "  'hunger',\n",
       "  'stricken',\n",
       "  'counti',\n",
       "  'desper',\n",
       "  'appeal',\n",
       "  'intern',\n",
       "  'agenc',\n",
       "  'intensifi',\n",
       "  'short',\n",
       "  'whole',\n",
       "  'communiti',\n",
       "  'danger',\n",
       "  'cholera',\n",
       "  'mening',\n",
       "  'outbreak',\n",
       "  'note',\n",
       "  'messag',\n",
       "  'contribut',\n",
       "  'cash',\n",
       "  'wfp',\n",
       "  'purchas',\n",
       "  'farmer',\n",
       "  '70',\n",
       "  'develop',\n",
       "  'combat',\n",
       "  'mother',\n",
       "  'breast',\n",
       "  'until',\n",
       "  'age',\n",
       "  'hygien',\n",
       "  'china',\n",
       "  'week',\n",
       "  'least',\n",
       "  'miss',\n",
       "  'break',\n",
       "  'rail',\n",
       "  'link',\n",
       "  'ministri',\n",
       "  'affair',\n",
       "  'urgent',\n",
       "  'warm',\n",
       "  'wood',\n",
       "  'purpos',\n",
       "  'translat',\n",
       "  'part',\n",
       "  'us',\n",
       "  'partnership',\n",
       "  'intend',\n",
       "  'stockpil',\n",
       "  'shelf',\n",
       "  'stabl',\n",
       "  'address',\n",
       "  'ongo',\n",
       "  'insecur',\n",
       "  'drop',\n",
       "  'although',\n",
       "  'stem',\n",
       "  'grasshopp',\n",
       "  'crop',\n",
       "  'concern',\n",
       "  'transmit',\n",
       "  'know',\n",
       "  'ou',\n",
       "  'medium',\n",
       "  'suspect',\n",
       "  'crime',\n",
       "  'hand',\n",
       "  'foot',\n",
       "  'addit',\n",
       "  'idp',\n",
       "  'resettl',\n",
       "  'receiv',\n",
       "  'packag',\n",
       "  'amount',\n",
       "  'instal',\n",
       "  'telephon',\n",
       "  'institut',\n",
       "  'electr',\n",
       "  'substat',\n",
       "  'damag',\n",
       "  'idea',\n",
       "  'await',\n",
       "  'monsoon',\n",
       "  'shift',\n",
       "  'gear',\n",
       "  'intens',\n",
       "  'period',\n",
       "  'torrenti',\n",
       "  'moder',\n",
       "  'upper',\n",
       "  'reach',\n",
       "  'indus',\n",
       "  'spell',\n",
       "  'troubl',\n",
       "  'tributari',\n",
       "  'flow',\n",
       "  'maximum',\n",
       "  'capac',\n",
       "  'chariti',\n",
       "  'step',\n",
       "  'fill',\n",
       "  'gap',\n",
       "  'weaken',\n",
       "  'activ',\n",
       "  'new',\n",
       "  'ebola',\n",
       "  'joint',\n",
       "  'old',\n",
       "  'male',\n",
       "  'anyon',\n",
       "  'young',\n",
       "  'girl',\n",
       "  'almost',\n",
       "  'everi',\n",
       "  'hour',\n",
       "  'tremor',\n",
       "  'feel',\n",
       "  'measur',\n",
       "  'richter',\n",
       "  'scale',\n",
       "  'night',\n",
       "  'subscrib',\n",
       "  'serv',\n",
       "  'possibl',\n",
       "  'coat',\n",
       "  'jacket',\n",
       "  'glove',\n",
       "  'hat',\n",
       "  'read',\n",
       "  'yon',\n",
       "  'act',\n",
       "  'tour',\n",
       "  'dure',\n",
       "  'raini',\n",
       "  'season',\n",
       "  'septemb',\n",
       "  'locat',\n",
       "  'suffer',\n",
       "  '2',\n",
       "  'tweet',\n",
       "  'complaint',\n",
       "  'think',\n",
       "  't',\n",
       "  'cold',\n",
       "  'front',\n",
       "  'cuba',\n",
       "  'morn',\n",
       "  'haiti',\n",
       "  'tomorrow',\n",
       "  'isol',\n",
       "  'shower',\n",
       "  'region',\n",
       "  'mom',\n",
       "  'fifth',\n",
       "  'crash',\n",
       "  'june',\n",
       "  'job',\n",
       "  'end',\n",
       "  'leg',\n",
       "  '80',\n",
       "  'weekend',\n",
       "  'exacerb',\n",
       "  'frustrat',\n",
       "  'plan',\n",
       "  'role',\n",
       "  'temporari',\n",
       "  'perman',\n",
       "  'adequ',\n",
       "  'sanit',\n",
       "  'inform',\n",
       "  'structur',\n",
       "  'urban',\n",
       "  'four',\n",
       "  'union',\n",
       "  'within',\n",
       "  'saint',\n",
       "  'martin',\n",
       "  'current',\n",
       "  'experi',\n",
       "  'between',\n",
       "  'low',\n",
       "  'lie',\n",
       "  'wind',\n",
       "  '200',\n",
       "  'kilometr',\n",
       "  'per',\n",
       "  'meet',\n",
       "  '50,000',\n",
       "  'worst',\n",
       "  'assam',\n",
       "  'bihar',\n",
       "  'shelter',\n",
       "  'pack',\n",
       "  'improv',\n",
       "  'qualiti',\n",
       "  'prepar',\n",
       "  'term',\n",
       "  'necessari',\n",
       "  'mitig',\n",
       "  'further',\n",
       "  'six',\n",
       "  'phase',\n",
       "  'proper',\n",
       "  'household',\n",
       "  'project',\n",
       "  'canal',\n",
       "  'boi',\n",
       "  '4th',\n",
       "  'section',\n",
       "  'rout',\n",
       "  'enter',\n",
       "  'big',\n",
       "  'mountain',\n",
       "  'approxim',\n",
       "  '300,000',\n",
       "  'i.e.',\n",
       "  'stone',\n",
       "  'corrug',\n",
       "  'iron',\n",
       "  'onli',\n",
       "  '30',\n",
       "  'construct',\n",
       "  'wait',\n",
       "  'cousin',\n",
       "  'leogan',\n",
       "  'correct',\n",
       "  '75',\n",
       "  'anyth',\n",
       "  'open',\n",
       "  'noth',\n",
       "  'eat',\n",
       "  'stro_soichi',\n",
       "  'http://twitpic.com/15wu5u',\n",
       "  'santiago',\n",
       "  'capit',\n",
       "  'mega',\n",
       "  'earthquake(m8.8',\n",
       "  'glass',\n",
       "  'wine',\n",
       "  'block',\n",
       "  'railway',\n",
       "  'choke',\n",
       "  'coal',\n",
       "  'shipment',\n",
       "  'energi',\n",
       "  '17',\n",
       "  'status',\n",
       "  'result',\n",
       "  'survey',\n",
       "  'indic',\n",
       "  'preval',\n",
       "  'global',\n",
       "  'percent',\n",
       "  'correspond',\n",
       "  'rate',\n",
       "  '2.4',\n",
       "  'dad',\n",
       "  'knock',\n",
       "  'rest',\n",
       "  'fun',\n",
       "  'pier',\n",
       "  'devast',\n",
       "  'scholarship',\n",
       "  'immigr',\n",
       "  'seneg',\n",
       "  'great',\n",
       "  'busi',\n",
       "  'kachipul',\n",
       "  'bag',\n",
       "  'just',\n",
       "  'jersey',\n",
       "  'pa',\n",
       "  'problem',\n",
       "  'famin',\n",
       "  'increas',\n",
       "  'learn',\n",
       "  'talk',\n",
       "  'heart',\n",
       "  'resourc',\n",
       "  'free',\n",
       "  'compani',\n",
       "  'shanghai',\n",
       "  'volunt',\n",
       "  'outdoor',\n",
       "  'fundrais',\n",
       "  'subway',\n",
       "  'station',\n",
       "  'magnitud',\n",
       "  'western',\n",
       "  'coast',\n",
       "  'wreak',\n",
       "  'nia',\n",
       "  'yesterday',\n",
       "  'march',\n",
       "  '28',\n",
       "  '2005',\n",
       "  'anywher',\n",
       "  '300',\n",
       "  '2000',\n",
       "  'dead',\n",
       "  'earli',\n",
       "  'estim',\n",
       "  '25,000',\n",
       "  'displac',\n",
       "  'widespread',\n",
       "  'critic',\n",
       "  'interim',\n",
       "  'conven',\n",
       "  'war',\n",
       "  'justic',\n",
       "  'ahmad',\n",
       "  'basti',\n",
       "  'ghulam',\n",
       "  'ali',\n",
       "  'kot',\n",
       "  'addu',\n",
       "  'good',\n",
       "  'switzerland',\n",
       "  'team',\n",
       "  '45',\n",
       "  'dog',\n",
       "  '27th',\n",
       "  '2001',\n",
       "  'american',\n",
       "  'upgrad',\n",
       "  'solid',\n",
       "  'wast',\n",
       "  'manag',\n",
       "  'seven',\n",
       "  'indonesia',\n",
       "  'sit',\n",
       "  'pacif',\n",
       "  'ring',\n",
       "  ...],\n",
       " [],\n",
       " ['prevent', 'exact'],\n",
       " ['kcna', 'democrat', '14,000', 'headach', 'laplain'],\n",
       " [],\n",
       " ['crazi'],\n",
       " ['14,000', 'gender'],\n",
       " ['sud'],\n",
       " [],\n",
       " [],\n",
       " ['mayb', 'prcs'],\n",
       " ['3.5', '30,000', 'seawat'],\n",
       " ['lay', 'laplain'],\n",
       " ['twin'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['kit'],\n",
       " ['detect'],\n",
       " [],\n",
       " ['new', 'bulldoz', '85'],\n",
       " [],\n",
       " ['week'],\n",
       " ['jacket'],\n",
       " ['give'],\n",
       " ['command'],\n",
       " ['actor'],\n",
       " [],\n",
       " ['burn',\n",
       "  'limit',\n",
       "  'fight',\n",
       "  'archipelago',\n",
       "  'though',\n",
       "  'unpreced',\n",
       "  'logist',\n",
       "  'nyu',\n",
       "  'wen',\n",
       "  'uttarakhand'],\n",
       " ['though', 'vari', 'respect', 'adjac'],\n",
       " ['alreadi', 'logist', 'pneumonia', 'nyu', 'autonomi'],\n",
       " ['ministri', 'pupil', 'bone'],\n",
       " ['burn', 'archipelago', 'unpreced', 'wen'],\n",
       " ['drastic'],\n",
       " ['blow'],\n",
       " []]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canonTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canon = dict(pd.Series(data=canonTable, index=out_columns, name='Canon Tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('canon.joblib', 'wb') as f:\n",
    "    joblib.dump(canonTable, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
