{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract The Tokens Most Associated With Each Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve ML model that has been cached to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # tokenize the text using spacy's model for English\n",
    "    doc = en_nlp(text)\n",
    "    # while we lemmatize the now tokenized text, let's not forget to drop\n",
    "    #   tokens that are stop_words or punctuation\n",
    "    lemmas = [token.lemma_ for token in doc\n",
    "        if token not in stopwords and not token.is_punct]\n",
    "    # Had better luck with this nltk stemmer\n",
    "    return [stemmer.stem(lemma) for lemma in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../models/ml_model.pkl', 'rb') as f:\n",
    "#     ml_model = joblib.load(f)\n",
    "    \n",
    "# with open('../models/nlp_model.pkl', 'rb') as f:\n",
    "#     nlp_model = joblib.load(f)\n",
    "    \n",
    "with open('../models/classifier.pkl', 'rb') as f:\n",
    "    model = joblib.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model = model['tfidfvectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = model['multioutputclassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract The Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there some other way we could get the vocabulary?  Some way where we don't have to reconstruct and train the vectorizer?\n",
    "- Can we store the vectorizer like we store the model?  Then we could just read it from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = vect.vocabulary_\n",
    "# vocab = list(nlp_model['tfidfvectorizer'].vocabulary_.keys())\n",
    "vocab = list(nlp_model.vocabulary_.keys())\n",
    "# No good.  vocabulary_ is a dict, and dicts are only ordered in recent versions of python.  \n",
    "# We can't rely on the dict to return the tokens in the correct order.\n",
    "# I've gone ahead even though I'm still concerned about this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab)\n",
    "n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nlp_vocabulary.joblib', 'wb') as f:\n",
    "    joblib.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Canon Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we're up to...   For every token in the vocabulary we're going to take its vector representation (a single 1 and a lot of 0s) and feed it to the pipeline to get a prediction.  That prediction becomes a row with 36 elements which is appended to a big table which winds up with 4859 rows.  We get a big table 4859 x 36.\n",
    "\n",
    "From this table we then extract the columns.  For each non-zero entry in a column we look up the associated token.  We call these tokens the \"canon tokens\".  \n",
    "\n",
    "What we are doing is a crude kind of matrix inversion, which would give us our features perfectly if the pipeline were a strictly linear system, but of course it isn't.  But it's the best we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTokenTable(model, n_vocab, n_categories):\n",
    "    result = np.zeros(shape = (n_vocab, n_categories))\n",
    "    for i in range(n_vocab):\n",
    "        # construct token vector for single token\n",
    "        tokenVec = np.zeros(shape=n_vocab)\n",
    "        tokenVec[i] = 1\n",
    "        # compute categories for that single token and append to table\n",
    "        result[i] = model.predict([tokenVec])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Rewrite `genTokenTable()` to use csr (or lil) rather than np.array.~~  Ok. That was a terrible idea.  It's non-trivial to use sparse.lil arrays with the exceptionally useful `numpy.where()` function.  Let's go back to the original method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def genTokenTable(n_vocab):\n",
    "#     result = sparse.lil_matrix((n_vocab, 36), dtype=int)\n",
    "#     for i in range(n_vocab):\n",
    "#         # construct token vector for single token\n",
    "#         tokenVec = np.zeros(shape=n_vocab)\n",
    "#         tokenVec[i] = 1\n",
    "#         # compute categories for that single token and append to table\n",
    "#         result[i,:] = pipe.predict([tokenVec])\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenTable = genTokenTable(clf_model, n_vocab, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenTable.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genCanonTable(table, vocab):\n",
    "    result = []\n",
    "    for i in range(table.shape[1]):\n",
    "        categoryVector = tokenTable[:,i]\n",
    "        tokenIndices = np.where(categoryVector == 1)[0]\n",
    "        # A wily trick for indexing a list\n",
    "        result.append(list(np.array(vocab)[tokenIndices]))\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canonTable = [canonTokens(tokenTable, i) for i in range(36)]\n",
    "canonTable = genCanonTable(tokenTable, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canon = dict(pd.Series(data=canonTable, index=out_columns, name='Canon Tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('canon.joblib', 'wb') as f:\n",
    "    joblib.dump(canonTable, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
