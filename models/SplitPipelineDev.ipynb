{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal -\n",
    "    Test initiative re how pipeline is constructed and trained in production, i.e. in train_classifer.py\n",
    "\n",
    "### Current Initiative -\n",
    "    Construct the pipeline in two pieces: 1 the nlp vectorizer piece and 2 the ML model. Cache intermediates when training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from database file in `data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sqal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle as pkl\n",
    "import joblib\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    # open the database file created by previous script\n",
    "    engine = sqal.create_engine('sqlite:///' + database_filepath)\n",
    "    # and grab the table therein\n",
    "    df = pd.read_sql_table('MessageCategorization', engine)\n",
    "\n",
    "    in_columns = 'message'\n",
    "    out_columns = list(df.columns)[4:]\n",
    "\n",
    "    # remove outliers from 'related' column\n",
    "    df['related'] = np.clip(df['related'], 0, 1)\n",
    "\n",
    "    text = df[in_columns].values\n",
    "    y = df[out_columns].values\n",
    "\n",
    "    # # save some for data for testing the trained model\n",
    "    # text_train, text_test, y_train, y_test = \\\n",
    "    #     train_test_split(text, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    return text, y, out_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, y, out_columns = load_data('../data/DisasterResponse.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = train_test_split(text, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # tokenize the text using spacy's model for English\n",
    "    doc = en_nlp(text)\n",
    "    # while we lemmatize the now tokenized text, let's not forget to drop\n",
    "    #   tokens that are stop_words or punctuation\n",
    "    lemmas = [token.lemma_ for token in doc\n",
    "        if token not in stopwords and not token.is_punct]\n",
    "    # Had better luck with this nltk stemmer\n",
    "    return [stemmer.stem(lemma) for lemma in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model = make_pipeline(\n",
    "    TfidfVectorizer(tokenizer=tokenize, min_df=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = make_pipeline(\n",
    "    MultiOutputClassifier(\n",
    "        estimator=AdaBoostClassifier(\n",
    "            base_estimator=DecisionTreeClassifier(max_depth=2),\n",
    "            n_estimators=10, learning_rate=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the two pieces of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 23s, sys: 2.42 s, total: 3min 26s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = nlp_model.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.7 s, sys: 230 ms, total: 31.9 s\n",
      "Wall time: 35 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('multioutputclassifier',\n",
       "                 MultiOutputClassifier(estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),\n",
       "                                                                    learning_rate=1,\n",
       "                                                                    n_estimators=10)))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ml_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 1.46 s, total: 1min 47s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = nlp_model.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 729 ms, sys: 16.7 ms, total: 745 ms\n",
      "Wall time: 756 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = ml_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tasks we need to cache the model or it's products for are:\n",
    "1. score the model\n",
    "    - requires `y_test` and `y_pred`\n",
    "2. generate the scatter plot with f1 scores vs. num per category\n",
    "    - requires `y_test`, `y_pred` and `num_pos`\n",
    "3. generate the bar chart with hovers listing relevant tokens for each category\n",
    "    - requires `num_pos` and `canonTable`\n",
    "4. compute predicted categories for novel messages\n",
    "    - requires `model.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `y_predicted` is cached on disk and read by `run.py`.\n",
    "- `num_pos` is computed from df in `run.py`\n",
    "- `df` is read from `DisasterResponse.db`\n",
    "- `canonTable` can be cached to disk, and read by `run.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tfidfvectorizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-bae63919589c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tfidfvectorizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# Not an int, try get step by name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tfidfvectorizer'"
     ]
    }
   ],
   "source": [
    "model['tfidfvectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp['tfidfvectorizer'].vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab)\n",
    "n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use use, in particular can we cache, the combined pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(nlp_model, ml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.get_params of Pipeline(steps=[('pipeline-1',\n",
       "                 Pipeline(steps=[('tfidfvectorizer',\n",
       "                                  TfidfVectorizer(min_df=5,\n",
       "                                                  tokenizer=<function tokenize at 0x131a3a3a0>))])),\n",
       "                ('pipeline-2',\n",
       "                 Pipeline(steps=[('multioutputclassifier',\n",
       "                                  MultiOutputClassifier(estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),\n",
       "                                                                                     learning_rate=1,\n",
       "                                                                                     n_estimators=10)))]))])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A positive development is that cooling has been restored to the reactor pressure vessels in Units 5 and 6.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([text_test[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
