{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal -\n",
    "    Test initiative re how pipeline is constructed and trained in production, i.e. in train_classifer.py\n",
    "\n",
    "### Current Initiative -\n",
    "    Construct the pipeline in two pieces: 1 the nlp vectorizer piece and 2 the ML model. Cache intermediates when training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from database file in `data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sqal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle as pkl\n",
    "import joblib\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    # open the database file created by previous script\n",
    "    engine = sqal.create_engine('sqlite:///' + database_filepath)\n",
    "    # and grab the table therein\n",
    "    df = pd.read_sql_table('MessageCategorization', engine)\n",
    "\n",
    "    in_columns = 'message'\n",
    "    out_columns = list(df.columns)[4:]\n",
    "\n",
    "    # remove outliers from 'related' column\n",
    "    df['related'] = np.clip(df['related'], 0, 1)\n",
    "\n",
    "    text = df[in_columns].values\n",
    "    y = df[out_columns].values\n",
    "\n",
    "    # # save some for data for testing the trained model\n",
    "    # text_train, text_test, y_train, y_test = \\\n",
    "    #     train_test_split(text, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    return text, y, out_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, y, out_columns = load_data('../data/DisasterResponse.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = train_test_split(text, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # tokenize the text using spacy's model for English\n",
    "    doc = en_nlp(text)\n",
    "    # while we lemmatize the now tokenized text, let's not forget to drop\n",
    "    #   tokens that are stop_words or punctuation\n",
    "    lemmas = [token.lemma_ for token in doc\n",
    "        if token not in stopwords and not token.is_punct]\n",
    "    # Had better luck with this nltk stemmer\n",
    "    return [stemmer.stem(lemma) for lemma in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model = make_pipeline(\n",
    "    TfidfVectorizer(tokenizer=tokenize, min_df=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = make_pipeline(\n",
    "    MultiOutputClassifier(\n",
    "        estimator=AdaBoostClassifier(\n",
    "            base_estimator=DecisionTreeClassifier(max_depth=2),\n",
    "            n_estimators=10, learning_rate=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the two pieces of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = nlp_model.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ml_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test = nlp_model.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = ml_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tasks we need to cache the model or it's products for are:\n",
    "1. score the model\n",
    "    - requires `y_test` and `y_pred`\n",
    "2. generate the scatter plot with f1 scores vs. num per category\n",
    "    - requires `y_test`, `y_pred` and `num_pos`\n",
    "3. generate the bar chart with hovers listing relevant tokens for each category\n",
    "    - requires `num_pos` and `canonTable`\n",
    "4. compute predicted categories for novel messages\n",
    "    - requires `model.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `y_predicted` is cached on disk and read by `run.py`.\n",
    "- `num_pos` is computed from df in `run.py`\n",
    "- `df` is read from `DisasterResponse.db`\n",
    "- `canonTable` can be cached to disk, and read by `run.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model['tfidfvectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp_model['tfidfvectorizer'].vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab)\n",
    "n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct The Combined Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use use, in particular can we cache, the combined pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(nlp_model, ml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([text_test[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/classifier.pkl', 'wb') as f:\n",
    "    joblib.dump(model, f)\n",
    "    \n",
    "with open('../models/nlp_model.pkl', 'wb') as f:\n",
    "    joblib.dump(nlp_model, f)\n",
    "\n",
    "with open('../models/ml_model.pkl', 'wb') as f:\n",
    "    joblib.dump(ml_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
